<?php

return [
    'Using Snappy compression significantly reduces storage size without much CPU overhead.',
    'In Hadoop, enabling compression on intermediate MapReduce outputs can improve job performance.',
    'Gzip offers better compression ratios but is slower to decompress than Snappy or LZO.',
    'For big data, compression helps reduce disk I/O and network transfer times.',
    'Bzip2 has excellent compression but is often too slow for real-time processing needs.',
    'Choose compression algorithms based on workload — speed vs. size tradeoff.',
    'Spark supports several codecs out of the box like LZ4, Snappy, and ZSTD.',
    'Columnar formats like Parquet and ORC work best with compression enabled.',
    'We saw 40% less storage usage when using Parquet + Snappy for analytics data.',
    'Avoid compressing already compressed formats — it adds no value and may hurt performance.',
    'Compression can reduce memory pressure during shuffles in Spark applications.',
    'Always benchmark with your real-world data to choose the best codec.',
    'Hadoop allows you to configure compression via core-site.xml and mapred-site.xml.',
    'Compression speeds up data transfer in distributed systems, especially in cloud storage.',
    'Using LZO requires native libraries — make sure they’re installed properly.',
    'Snappy is widely used in Kafka because of its fast compression and decompression.',
    'Compression changes CPU vs. I/O balance — monitor your bottlenecks.',
    'Compressing log files before ingestion saves significant bandwidth.',
    'Enable compression only if it improves throughput, not just to save space.',
    'In Hive, use `SET hive.exec.compress.output=true` to enable compression.',
    'Use Splittable compression formats to enable better parallelism.',
    'For archival, ZSTD or Bzip2 is great, even if slower, due to better size reduction.',
    'ORC + ZLIB in Hive gave us the best query performance with the least disk usage.',
    'Streaming workloads often prefer Snappy for speed over compactness.',
    'Benchmarking compression should include both write and read performance.',
    'Data skew may affect how well compression performs across partitions.',
    'Compression is not just for storage — it impacts serialization/deserialization too.',
    'Evaluate compression in your ETL pipeline end-to-end, not in isolation.',
    'Spark’s `spark.io.compression.codec` controls shuffle compression.',
    'Try compressing only cold data to balance performance and cost.',
    'Combining compression with bucketing or partitioning gives huge speedups.',
    'Many ML pipelines compress intermediate data for pipeline stages.',
    'If CPU is cheap and I/O is expensive, compression helps a lot.',
    'Test with real data sizes — small datasets often compress differently.',
    'Set up alerts for compression ratios dropping — may indicate bad input.',
    'Spark SQL lets you set codec per write operation — use it wisely.',
    'We gained 3x faster disk reads by switching from Gzip to LZ4.',
    'Good compression improves cloud egress costs significantly.',
    'Use `hadoop fs -du` to estimate how compression affects storage.',
    'Compression is critical when syncing large backups to cloud storage.'
];